import streamlit as st
import os
import glob
from typing import List, Dict, Any, Optional
import tempfile
from pathlib import Path
import json
from datetime import datetime
import base64
import hashlib

# API Key ç®¡ç†æ¨¡å—
CONFIG_FILE = os.path.join(".", ".deepseek_config.json")

def encode_api_key(api_key: str) -> str:
    """ç®€å•çš„ç¼–ç ï¼ˆBase64ï¼‰ï¼Œä¸æ˜¯çœŸæ­£çš„åŠ å¯†ï¼Œä½†å¯ä»¥é¿å…å®Œå…¨æ˜æ–‡"""
    if not api_key:
        return ""
    # ä½¿ç”¨ Base64 ç¼–ç 
    encoded = base64.b64encode(api_key.encode('utf-8')).decode('utf-8')
    return encoded

def decode_api_key(encoded_key: str) -> str:
    """è§£ç  API key"""
    if not encoded_key:
        return ""
    try:
        decoded = base64.b64decode(encoded_key.encode('utf-8')).decode('utf-8')
        return decoded
    except Exception:
        return ""

def save_api_key(api_key: str, show_error: bool = True) -> bool:
    """ä¿å­˜ API key åˆ°æœ¬åœ°é…ç½®æ–‡ä»¶"""
    try:
        config = {
            "api_key": encode_api_key(api_key),
            "saved_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        os.makedirs(os.path.dirname(CONFIG_FILE) if os.path.dirname(CONFIG_FILE) else ".", exist_ok=True)
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        if show_error and 'st' in globals():
            st.error(f"ä¿å­˜ API key å¤±è´¥: {str(e)}")
        return False

def load_api_key() -> Optional[str]:
    """ä»æœ¬åœ°é…ç½®æ–‡ä»¶åŠ è½½ API key"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = decode_api_key(config.get("api_key", ""))
                if api_key:
                    return api_key
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œå¦‚æœæ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨ï¼Œè¿”å› None
        pass
    return None

def delete_api_key(show_error: bool = True) -> bool:
    """åˆ é™¤æœ¬åœ°ä¿å­˜çš„ API key"""
    try:
        if os.path.exists(CONFIG_FILE):
            os.remove(CONFIG_FILE)
            return True
        return False
    except Exception as e:
        if show_error and 'st' in globals():
            st.error(f"åˆ é™¤ API key å¤±è´¥: {str(e)}")
        return False

# æ–‡ä»¶è¯»å–æ¨¡å—
def read_text_file(file_path):
    """è¯»å–txtæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def read_docx_file(file_path):
    """è¯»å–Wordæ–‡æ¡£"""
    try:
        from docx import Document
        doc = Document(file_path)
        return '\n'.join([para.text for para in doc.paragraphs])
    except ImportError:
        return "è¯·å®‰è£…python-docx: pip install python-docx"

def read_pdf_file(file_path):
    """è¯»å–PDFæ–‡ä»¶"""
    try:
        from pypdf import PdfReader
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"PDFè¯»å–å¤±è´¥: {str(e)}"

def read_excel_file(file_path):
    """è¯»å–Excelæ–‡ä»¶"""
    try:
        import pandas as pd
        excel_content = {}
        xls = pd.ExcelFile(file_path)
        for sheet_name in xls.sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            excel_content[sheet_name] = df.to_string()
        return excel_content
    except ImportError:
        return {"é”™è¯¯": "è¯·å®‰è£…pandaså’Œopenpyxl"}

def process_folder(folder_path: str) -> Dict[str, Any]:
    """å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    all_docs = {}
    
    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    file_patterns = {
        '*.txt': ('txt', read_text_file),
        '*.docx': ('docx', read_docx_file),
        '*.pdf': ('pdf', read_pdf_file),
        '*.xlsx': ('excel', read_excel_file),
        '*.xls': ('excel', read_excel_file),
    }
    
    for pattern, (file_type, reader_func) in file_patterns.items():
        for file_path in glob.glob(os.path.join(folder_path, pattern)):
            file_name = os.path.basename(file_path)
            try:
                content = reader_func(file_path)
                all_docs[file_name] = {
                    'path': file_path,
                    'content': content,
                    'type': file_type,
                    'size': os.path.getsize(file_path)
                }
            except Exception as e:
                all_docs[file_name] = {
                    'path': file_path,
                    'content': f"è¯»å–å¤±è´¥: {str(e)}",
                    'type': 'error',
                    'size': 0
                }
    
    return all_docs

# æœ¬åœ°å‘é‡æ•°æ®åº“æ¨¡å—ï¼ˆä¸éœ€è¦APIå¯†é’¥ï¼‰
def get_model_path(model_name: str = "BAAI/bge-small-zh-v1.5") -> str:
    """è·å–æ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
    
    Args:
        model_name: HuggingFace æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
    
    Returns:
        æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™è¿”å›æ¨¡å‹åç§°ï¼‰
    """
    import os
    
    try:
        # å¦‚æœå·²ç»æ˜¯æœ¬åœ°è·¯å¾„ä¸”å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(model_name) and os.path.isdir(model_name):
            return model_name
        
        # æ£€æŸ¥ HuggingFace ç¼“å­˜ç›®å½•
        cache_dir = os.path.join(
            os.path.expanduser("~"),
            ".cache",
            "huggingface",
            "hub"
        )
        
        # å°†æ¨¡å‹åç§°è½¬æ¢ä¸ºç¼“å­˜ç›®å½•æ ¼å¼ï¼ˆBAAI/bge-small-zh-v1.5 -> models--BAAI--bge-small-zh-v1.5ï¼‰
        cache_model_name = f"models--{model_name.replace('/', '--')}"
        cache_path = os.path.join(cache_dir, cache_model_name)
        
        # æŸ¥æ‰¾ snapshots ç›®å½•ä¸‹çš„æœ€æ–°ç‰ˆæœ¬
        if os.path.exists(cache_path):
            snapshots_dir = os.path.join(cache_path, "snapshots")
            if os.path.exists(snapshots_dir):
                try:
                    # è·å–æ‰€æœ‰å¿«ç…§ç‰ˆæœ¬
                    snapshots = [d for d in os.listdir(snapshots_dir) 
                                if os.path.isdir(os.path.join(snapshots_dir, d))]
                    if snapshots:
                        # ä½¿ç”¨æœ€æ–°çš„å¿«ç…§
                        latest_snapshot = sorted(snapshots)[-1]
                        local_path = os.path.join(snapshots_dir, latest_snapshot)
                        if os.path.exists(local_path):
                            return local_path
                except (OSError, PermissionError):
                    pass  # å¿½ç•¥è®¿é—®é”™è¯¯ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–è·¯å¾„
        
        # æ£€æŸ¥é¡¹ç›®ç›®å½•ä¸‹çš„ models æ–‡ä»¶å¤¹
        project_model_path = os.path.join(".", "models", model_name.replace("/", "--"))
        if os.path.exists(project_model_path):
            return project_model_path
    except Exception:
        pass  # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œè¿”å›åŸå§‹æ¨¡å‹åç§°
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹æ¨¡å‹åç§°ï¼ˆä¼šè§¦å‘ä¸‹è½½ï¼‰
    return model_name

def check_db_corrupted(db_path: str) -> bool:
    """æ£€æµ‹å‘é‡æ•°æ®åº“æ˜¯å¦æŸåï¼ˆç‰¹åˆ«æ˜¯ schema å…¼å®¹æ€§é—®é¢˜ï¼‰
    
    Args:
        db_path: å‘é‡æ•°æ®åº“è·¯å¾„
    
    Returns:
        True å¦‚æœæ•°æ®åº“æŸåï¼ŒFalse å¦‚æœæ­£å¸¸æˆ–ä¸å­˜åœ¨
    """
    if not os.path.exists(db_path):
        return False
    
    try:
        # å°è¯•åŠ è½½æ•°æ®åº“æ¥æ£€æµ‹æ˜¯å¦æŸå
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
        except ImportError:
            from langchain_community.embeddings import HuggingFaceEmbeddings
        
        try:
            from langchain_chroma import Chroma
        except ImportError:
            from langchain_community.vectorstores import Chroma
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        model_path = get_model_path("BAAI/bge-small-zh-v1.5")
        embeddings = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # å°è¯•åŠ è½½å‘é‡æ•°æ®åº“
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )
        
        # å°è¯•è®¿é—®æ•°æ®åº“ï¼ˆå¦‚æœ schema é”™è¯¯ä¼šåœ¨è¿™é‡Œå¤±è´¥ï¼‰
        _ = len(vectorstore)
        return False  # æ•°æ®åº“æ­£å¸¸
    except Exception as e:
        error_msg = str(e).lower()
        # æ£€æµ‹å¸¸è§çš„æ•°æ®åº“é”™è¯¯
        is_corrupted = (
            "no such column" in error_msg or
            "collections.topic" in error_msg or
            "hnsw" in error_msg or
            "index" in error_msg or
            "compaction" in error_msg or
            "segment" in error_msg or
            "schema" in error_msg or
            "sqlite" in error_msg
        )
        if is_corrupted:
            print(f"âš ï¸ æ£€æµ‹åˆ°æ•°æ®åº“æŸå: {str(e)}")
        return is_corrupted

def cleanup_corrupted_db(db_path: str, force: bool = True):
    """å½»åº•æ¸…ç†æŸåçš„å‘é‡æ•°æ®åº“ç›®å½•
    
    Args:
        db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†ï¼ˆåŒ…æ‹¬å¤šæ¬¡å°è¯•å’Œå»¶è¿Ÿï¼‰
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ¸…ç†
    """
    import shutil
    import time
    
    if not os.path.exists(db_path):
        return True
    
    if force:
        # å¼ºåˆ¶æ¸…ç†æ¨¡å¼ï¼šå¤šæ¬¡å°è¯•ï¼Œç¡®ä¿å½»åº•åˆ é™¤
        max_attempts = 5
        for attempt in range(max_attempts):
            try:
                # å…ˆå°è¯•æ­£å¸¸åˆ é™¤
                if os.path.exists(db_path):
                    shutil.rmtree(db_path, ignore_errors=False)
                
                # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»Ÿæ›´æ–°
                time.sleep(0.5)
                
                # éªŒè¯æ˜¯å¦åˆ é™¤æˆåŠŸ
                if not os.path.exists(db_path):
                    print(f"âœ… å·²å½»åº•æ¸…ç†æŸåçš„å‘é‡æ•°æ®åº“ç›®å½•: {db_path}")
                    return True
                    
            except PermissionError as pe:
                # Windows ä¸Šå¯èƒ½æœ‰æ–‡ä»¶è¢«é”å®šï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_attempts - 1:
                    print(f"âš ï¸ æ–‡ä»¶è¢«é”å®šï¼Œç­‰å¾…åé‡è¯• ({attempt + 1}/{max_attempts})...")
                    time.sleep(1)
                    continue
                else:
                    print(f"âŒ æ¸…ç†å¤±è´¥ï¼ˆæ–‡ä»¶è¢«é”å®šï¼‰: {str(pe)}")
                    print(f"   è¯·æ‰‹åŠ¨åˆ é™¤ç›®å½•: {db_path}")
                    return False
            except Exception as e:
                if attempt < max_attempts - 1:
                    print(f"âš ï¸ æ¸…ç†å¤±è´¥ï¼Œé‡è¯• ({attempt + 1}/{max_attempts}): {str(e)}")
                    time.sleep(0.5)
                    continue
                else:
                    print(f"âŒ æ¸…ç†å¤±è´¥: {str(e)}")
                    print(f"   è¯·æ‰‹åŠ¨åˆ é™¤ç›®å½•: {db_path}")
                    return False
        
        # å¦‚æœå¤šæ¬¡å°è¯•åä»ç„¶å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ ignore_errors
        if os.path.exists(db_path):
            try:
                shutil.rmtree(db_path, ignore_errors=True)
                time.sleep(0.5)
                if not os.path.exists(db_path):
                    print(f"âœ… å·²å¼ºåˆ¶æ¸…ç†æ•°æ®åº“ç›®å½•: {db_path}")
                    return True
            except Exception:
                pass
        
        # æœ€åå°è¯•ï¼šé‡å‘½åç›®å½•ï¼ˆå¦‚æœæ— æ³•åˆ é™¤ï¼‰
        if os.path.exists(db_path):
            try:
                import tempfile
                temp_name = db_path + "_deleted_" + str(int(time.time()))
                os.rename(db_path, temp_name)
                print(f"âš ï¸ æ— æ³•åˆ é™¤ç›®å½•ï¼Œå·²é‡å‘½åä¸º: {temp_name}")
                print(f"   è¯·ç¨åæ‰‹åŠ¨åˆ é™¤")
                return True
            except Exception as e:
                print(f"âŒ æ— æ³•æ¸…ç†ç›®å½•: {str(e)}")
                print(f"   è¯·æ‰‹åŠ¨åˆ é™¤: {db_path}")
                return False
        
        return False
    else:
        # ç®€å•æ¸…ç†æ¨¡å¼
        try:
            if os.path.exists(db_path):
                shutil.rmtree(db_path)
                print(f"âœ… å·²æ¸…ç†æ•°æ®åº“ç›®å½•: {db_path}")
                return True
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å¤±è´¥: {str(e)}")
            return False
    
    return False

def get_vector_db_path(folder_path: str) -> str:
    """æ ¹æ®æ–‡ä»¶å¤¹è·¯å¾„ç”Ÿæˆå”¯ä¸€çš„å‘é‡æ•°æ®åº“ç›®å½•è·¯å¾„
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        å‘é‡æ•°æ®åº“ç›®å½•è·¯å¾„
    """
    if not folder_path:
        # ä¸Šä¼ æ–‡ä»¶æ—¶æ²¡æœ‰æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        return "./chroma_db"
    
    # ä½¿ç”¨è·¯å¾„çš„å“ˆå¸Œå€¼åˆ›å»ºå”¯ä¸€ç›®å½•å
    # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ ï¼Œå¤„ç†å¤§å°å†™ï¼‰
    normalized_path = os.path.normpath(folder_path).replace('\\', '/')
    path_hash = hashlib.md5(normalized_path.encode('utf-8')).hexdigest()[:12]
    
    # åˆ›å»ºå®‰å…¨çš„ç›®å½•åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
    safe_folder_name = os.path.basename(normalized_path)
    safe_folder_name = "".join(c for c in safe_folder_name if c.isalnum() or c in (' ', '-', '_'))[:30]
    safe_folder_name = safe_folder_name.strip() or "unknown"
    
    # ç»„åˆç›®å½•åï¼šæ–‡ä»¶å¤¹å_å“ˆå¸Œå€¼
    db_dir_name = f"{safe_folder_name}_{path_hash}"
    return os.path.join("./chroma_db", db_dir_name)

def load_existing_vector_store(folder_path: str = None, progress_callback=None):
    """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç”¨äºç¡®å®šå‘é‡æ•°æ®åº“ä½ç½®ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (progress, message) å‚æ•°
    
    Returns:
        å‘é‡æ•°æ®åº“å¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™è¿”å› None
    """
    try:
        # ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬çš„åŒ…
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
        except ImportError:
            from langchain_community.embeddings import HuggingFaceEmbeddings
        
        try:
            from langchain_chroma import Chroma
        except ImportError:
            from langchain_community.vectorstores import Chroma
        
        db_path = get_vector_db_path(folder_path) if folder_path else "./chroma_db"
        
        if not os.path.exists(db_path):
            return None
        
        if progress_callback:
            progress_callback(10, "ğŸ”„ æ­£åœ¨åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“...")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¿…é¡»ä¸åˆ›å»ºæ—¶ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼‰
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œé¿å…ç½‘ç»œä¸‹è½½
        model_path = get_model_path("BAAI/bge-small-zh-v1.5")
        embeddings = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        if progress_callback:
            progress_callback(50, "ğŸ”„ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
        
        # ä»æŒä¹…åŒ–ç›®å½•åŠ è½½
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )
        
        # éªŒè¯å‘é‡æ•°æ®åº“æ˜¯å¦å¯ç”¨ï¼ˆå°è¯•è·å–æ•°é‡ï¼Œå¦‚æœç´¢å¼•æŸåä¼šåœ¨è¿™é‡Œå¤±è´¥ï¼‰
        try:
            _ = len(vectorstore)  # è¿™ä¼šè§¦å‘ count() è°ƒç”¨ï¼Œå¦‚æœç´¢å¼•æŸåä¼šæŠ›å‡ºå¼‚å¸¸
        except Exception as verify_error:
            # ç´¢å¼•æ–‡ä»¶å¯èƒ½æŸåï¼Œæ£€æµ‹æ˜¯å¦æ˜¯ schema é”™è¯¯
            error_msg = str(verify_error).lower()
            is_schema_error = (
                "no such column" in error_msg or
                "collections.topic" in error_msg or
                "schema" in error_msg
            )
            
            if is_schema_error:
                # Schema é”™è¯¯ï¼ˆç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œæ¸…ç†æ•°æ®åº“
                if progress_callback:
                    progress_callback(100, "âš ï¸ æ£€æµ‹åˆ°æ•°æ®åº“ schema é”™è¯¯ï¼ˆç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œæ­£åœ¨æ¸…ç†...")
                cleanup_corrupted_db(db_path, force=True)
            else:
                # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ç´¢å¼•æŸåï¼‰ï¼Œä¹Ÿæ¸…ç†
                if progress_callback:
                    progress_callback(100, "âš ï¸ å‘é‡æ•°æ®åº“ç´¢å¼•å¯èƒ½æŸåï¼Œå°†é‡æ–°åˆ›å»º...")
                cleanup_corrupted_db(db_path, force=True)
            return None
        
        if progress_callback:
            progress_callback(100, "âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆï¼")
        
        return vectorstore
    except Exception as e:
        # åŠ è½½å¤±è´¥ï¼Œè¿”å› None
        return None

def calculate_content_hash(content: Any) -> str:
    """è®¡ç®—æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼
    
    Args:
        content: æ–‡æ¡£å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
    
    Returns:
        å†…å®¹çš„ MD5 å“ˆå¸Œå€¼
    """
    if isinstance(content, dict):
        # Excel æ–‡ä»¶ï¼šåˆå¹¶æ‰€æœ‰å·¥ä½œè¡¨å†…å®¹
        content_str = "\n".join([f"{k}:{v}" for k, v in content.items()])
    else:
        content_str = str(content)
    
    return hashlib.md5(content_str.encode('utf-8')).hexdigest()

def check_docs_changed(docs_dict: Dict[str, Any], folder_path: str) -> bool:
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å‘ç”Ÿå˜åŒ–
    
    Args:
        docs_dict: å½“å‰æ–‡æ¡£å­—å…¸
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        True å¦‚æœæ–‡æ¡£å‘ç”Ÿå˜åŒ–ï¼ŒFalse å¦‚æœæœªå˜åŒ–
    """
    # åˆ›å»ºæ–‡æ¡£ç­¾åæ–‡ä»¶è·¯å¾„ï¼ˆåŸºäºæ–‡ä»¶å¤¹è·¯å¾„ï¼‰
    db_path = get_vector_db_path(folder_path)
    signature_file = os.path.join(db_path, ".docs_signature.json")
    
    if not os.path.exists(signature_file):
        return True  # ç­¾åæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè®¤ä¸ºæ–‡æ¡£å·²å˜åŒ–
    
    try:
        # è¯»å–ä¹‹å‰çš„ç­¾å
        with open(signature_file, 'r', encoding='utf-8') as f:
            old_signature = json.load(f)
        
        # ç”Ÿæˆå½“å‰æ–‡æ¡£ç­¾å
        current_signature = {
            "folder_path": folder_path,
            "file_count": len(docs_dict),
            "files": {}
        }
        
        for filename, data in docs_dict.items():
            file_path = data.get('path', '')
            content = data.get('content', '')
            
            file_info = {
                "size": data.get('size', 0),
                "content_hash": calculate_content_hash(content)  # ä½¿ç”¨å†…å®¹å“ˆå¸Œ
            }
            
            # å¦‚æœæ–‡ä»¶è·¯å¾„å­˜åœ¨ä¸”æ˜¯æŒä¹…è·¯å¾„ï¼ˆéä¸´æ—¶è·¯å¾„ï¼‰ï¼Œä¹Ÿè®°å½•ä¿®æ”¹æ—¶é—´
            if file_path and os.path.exists(file_path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è·¯å¾„ï¼ˆä¸´æ—¶è·¯å¾„é€šå¸¸åŒ…å« temp æˆ– tmpï¼‰
                is_temp_path = 'temp' in file_path.lower() or 'tmp' in file_path.lower()
                if not is_temp_path:
                    file_info["mtime"] = os.path.getmtime(file_path)
            
            current_signature["files"][filename] = file_info
        
        # æ¯”è¾ƒç­¾å
        if old_signature.get("folder_path") != current_signature["folder_path"]:
            return True
        
        if old_signature.get("file_count") != current_signature["file_count"]:
            return True
        
        old_files = old_signature.get("files", {})
        current_files = current_signature["files"]
        
        if set(old_files.keys()) != set(current_files.keys()):
            return True
        
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹å“ˆå¸Œï¼ˆä¼˜å…ˆï¼‰å’Œæ–‡ä»¶å¤§å°/ä¿®æ”¹æ—¶é—´
        for filename in old_files.keys():
            if filename not in current_files:
                return True
            
            old_info = old_files[filename]
            current_info = current_files[filename]
            
            # ä¼˜å…ˆä½¿ç”¨å†…å®¹å“ˆå¸Œè¿›è¡Œæ¯”è¾ƒï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            old_hash = old_info.get("content_hash")
            current_hash = current_info.get("content_hash")
            
            # å¦‚æœå½“å‰æœ‰å†…å®¹å“ˆå¸Œï¼Œä¼˜å…ˆä½¿ç”¨å“ˆå¸Œæ¯”è¾ƒ
            if current_hash:
                if old_hash:
                    # ä¸¤è€…éƒ½æœ‰å“ˆå¸Œå€¼ï¼Œç›´æ¥æ¯”è¾ƒ
                    if old_hash != current_hash:
                        return True
                    # å“ˆå¸Œå€¼ç›¸åŒï¼Œè®¤ä¸ºæ–‡æ¡£æœªå˜åŒ–ï¼ˆå³ä½¿ä¿®æ”¹æ—¶é—´ä¸åŒï¼‰
                    continue
                else:
                    # æ—§ç­¾åæ²¡æœ‰å“ˆå¸Œå€¼ï¼ˆå¯èƒ½æ˜¯æ—§ç‰ˆæœ¬ä¿å­˜çš„ï¼‰ï¼Œä½†å½“å‰æœ‰
                    # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ¯”è¾ƒæ–‡ä»¶å¤§å°ï¼Œä¸æ¯”è¾ƒä¿®æ”¹æ—¶é—´
                    # å› ä¸ºä¿®æ”¹æ—¶é—´å¯èƒ½å› ä¸ºå„ç§åŸå› å˜åŒ–ï¼ˆæ–‡ä»¶è¢«é‡æ–°ä¿å­˜ã€ç³»ç»Ÿæ—¶é—´è°ƒæ•´ç­‰ï¼‰
                    # ä½†æ–‡ä»¶å¤§å°ç›¸åŒé€šå¸¸æ„å‘³ç€å†…å®¹ç›¸åŒï¼ˆè™½ç„¶ä¸æ˜¯100%ç¡®å®šï¼Œä½†æ¦‚ç‡å¾ˆé«˜ï¼‰
                    if old_info.get("size") != current_info.get("size"):
                        return True
                    # æ–‡ä»¶å¤§å°ç›¸åŒï¼Œè®¤ä¸ºæ–‡æ¡£æœªå˜åŒ–ï¼ˆå³ä½¿ä¿®æ”¹æ—¶é—´ä¸åŒï¼‰
                    # ç­¾åæ–‡ä»¶ä¼šåœ¨ä¿å­˜æ—¶æ›´æ–°ï¼Œæ·»åŠ å†…å®¹å“ˆå¸Œï¼Œä¸‹æ¬¡æ¯”è¾ƒä¼šæ›´å‡†ç¡®
                    continue
            
            # å¦‚æœå½“å‰æ²¡æœ‰å†…å®¹å“ˆå¸Œï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
            # å›é€€åˆ°å¤§å°å’Œä¿®æ”¹æ—¶é—´æ¯”è¾ƒ
            if (old_info.get("size") != current_info.get("size") or 
                (old_info.get("mtime") and current_info.get("mtime") and
                 abs(old_info.get("mtime", 0) - current_info.get("mtime", 0)) > 1)):
                return True
        
        return False  # æ–‡æ¡£æœªå˜åŒ–
    except Exception as e:
        # è¯»å–ç­¾åå¤±è´¥ï¼Œè®¤ä¸ºæ–‡æ¡£å·²å˜åŒ–
        return True

def save_docs_signature(docs_dict: Dict[str, Any], folder_path: str):
    """ä¿å­˜æ–‡æ¡£ç­¾å
    
    Args:
        docs_dict: æ–‡æ¡£å­—å…¸
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
    """
    try:
        db_path = get_vector_db_path(folder_path)
        os.makedirs(db_path, exist_ok=True)
        signature_file = os.path.join(db_path, ".docs_signature.json")
        
        signature = {
            "folder_path": folder_path,
            "file_count": len(docs_dict),
            "files": {},
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        for filename, data in docs_dict.items():
            file_path = data.get('path', '')
            content = data.get('content', '')
            
            file_info = {
                "size": data.get('size', 0),
                "content_hash": calculate_content_hash(content)  # ä¿å­˜å†…å®¹å“ˆå¸Œ
            }
            
            # å¦‚æœæ–‡ä»¶è·¯å¾„å­˜åœ¨ä¸”æ˜¯æŒä¹…è·¯å¾„ï¼ˆéä¸´æ—¶è·¯å¾„ï¼‰ï¼Œä¹Ÿè®°å½•ä¿®æ”¹æ—¶é—´
            if file_path and os.path.exists(file_path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è·¯å¾„
                is_temp_path = 'temp' in file_path.lower() or 'tmp' in file_path.lower()
                if not is_temp_path:
                    file_info["mtime"] = os.path.getmtime(file_path)
            
            signature["files"][filename] = file_info
        
        with open(signature_file, 'w', encoding='utf-8') as f:
            json.dump(signature, f, indent=2, ensure_ascii=False)
    except Exception:
        pass  # ä¿å­˜ç­¾åå¤±è´¥ä¸å½±å“ä¸»æµç¨‹

def create_local_vector_store(docs_dict: Dict[str, Any], progress_callback=None, folder_path: str = None):
    """åˆ›å»ºæœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨å¼€æºåµŒå…¥æ¨¡å‹
    
    Args:
        docs_dict: æ–‡æ¡£å­—å…¸
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (progress, message) å‚æ•°
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç”¨äºç­¾åï¼‰
    """
    try:
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ langchain å¯¼å…¥
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
        except ImportError:
            try:
                from langchain.text_splitter import RecursiveCharacterTextSplitter
            except ImportError:
                from langchain_core.text_splitter import RecursiveCharacterTextSplitter
        
        # ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬çš„åŒ…
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
        except ImportError:
            from langchain_community.embeddings import HuggingFaceEmbeddings
        
        try:
            from langchain_chroma import Chroma
        except ImportError:
            from langchain_community.vectorstores import Chroma
        try:
            from langchain.schema import Document as LangDocument
        except ImportError:
            from langchain_core.documents import Document as LangDocument
        
        # æ­¥éª¤ 0: åœ¨å¼€å§‹å¤„ç†ä¹‹å‰ï¼Œå…ˆæ£€æµ‹å¹¶æ¸…ç†æŸåçš„æ•°æ®åº“ç›®å½•ï¼ˆé¿å…ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰
        db_path = get_vector_db_path(folder_path) if folder_path else "./chroma_db"
        
        # æ£€æŸ¥ç›®å½•æƒé™ï¼ˆåœ¨åˆ›å»ºç›®å½•ä¹‹å‰ï¼‰
        parent_dir = os.path.dirname(db_path) if os.path.dirname(db_path) else "."
        if not os.path.exists(parent_dir):
            os.makedirs(parent_dir, exist_ok=True)
        if not os.access(parent_dir, os.W_OK):
            raise PermissionError(f"æ²¡æœ‰å†™å…¥æƒé™: {parent_dir}")
        
        # å¦‚æœæ•°æ®åº“ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆæ£€æµ‹æ˜¯å¦æŸåï¼Œç„¶åæ¸…ç†ï¼ˆé¿å…ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰
        if os.path.exists(db_path):
            if progress_callback:
                progress_callback(5, "ğŸ”„ æ£€æµ‹å‘é‡æ•°æ®åº“çŠ¶æ€...")
            
            # å…ˆæ£€æµ‹æ•°æ®åº“æ˜¯å¦æŸåï¼ˆç‰¹åˆ«æ˜¯ schema å…¼å®¹æ€§é—®é¢˜ï¼‰
            is_corrupted = check_db_corrupted(db_path)
            
            if is_corrupted:
                if progress_callback:
                    progress_callback(5, "âš ï¸ æ£€æµ‹åˆ°æ•°æ®åº“æŸåï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œæ­£åœ¨æ¸…ç†...")
                cleanup_corrupted_db(db_path, force=True)
                import time
                time.sleep(1)  # ç­‰å¾…æ–‡ä»¶ç³»ç»Ÿæ›´æ–°
            else:
                # å³ä½¿æ£€æµ‹æ­£å¸¸ï¼Œå¦‚æœæ–‡æ¡£å˜åŒ–äº†ï¼Œä¹Ÿéœ€è¦æ¸…ç†é‡å»º
                # è¿™é‡Œå…ˆä¸æ¸…ç†ï¼Œè®©åç»­é€»è¾‘å¤„ç†
                pass
        
        # ç¡®ä¿æŸåçš„ç›®å½•è¢«æ¸…ç†
        if os.path.exists(db_path):
            # å†æ¬¡å°è¯•æ¸…ç†ï¼ˆé˜²æ­¢æ£€æµ‹é—æ¼ï¼‰
            try:
                # å¿«é€Ÿæ£€æµ‹ï¼šå¦‚æœç›®å½•å­˜åœ¨ä½†å¾ˆå°æˆ–ç»“æ„å¼‚å¸¸ï¼Œå¯èƒ½æ˜¯æŸåçš„
                import time
                if progress_callback:
                    progress_callback(5, "ğŸ”„ æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“ç›®å½•...")
                cleanup_corrupted_db(db_path, force=True)
                time.sleep(0.5)  # ç­‰å¾…æ–‡ä»¶ç³»ç»Ÿæ›´æ–°
            except Exception:
                pass
        
        # å¦‚æœä»ç„¶å­˜åœ¨ï¼Œå°è¯•é‡å‘½åï¼ˆæœ€åçš„æ‰‹æ®µï¼‰
        if os.path.exists(db_path):
            import time
            backup_name = db_path + "_backup_" + str(int(time.time()))
            try:
                os.rename(db_path, backup_name)
                print(f"âš ï¸ æ— æ³•åˆ é™¤ç›®å½•ï¼Œå·²é‡å‘½åä¸ºå¤‡ä»½: {backup_name}")
            except Exception:
                pass
        
        # æå–æ–‡æœ¬å†…å®¹
        if progress_callback:
            progress_callback(15, "ğŸ”„ æ­¥éª¤ 1/4: æå–æ–‡æ¡£å†…å®¹...")
        
        texts = []
        total_files = len(docs_dict)
        for idx, (filename, data) in enumerate(docs_dict.items()):
            content = data['content']
            if isinstance(content, dict):  # Excelæ–‡ä»¶
                for sheet, sheet_content in content.items():
                    texts.append(f"æ–‡ä»¶: {filename} | å·¥ä½œè¡¨: {sheet}\n{sheet_content}")
            else:
                texts.append(f"æ–‡ä»¶: {filename}\n{content}")
            
            if progress_callback and total_files > 0:
                progress = 15 + int((idx + 1) / total_files * 10)
                progress_callback(progress, f"ğŸ”„ æ­¥éª¤ 1/4: æå–æ–‡æ¡£å†…å®¹... ({idx + 1}/{total_files})")
        
        # åˆ†å‰²æ–‡æœ¬
        if progress_callback:
            progress_callback(30, "ğŸ”„ æ­¥éª¤ 2/4: åˆ†å‰²æ–‡æœ¬...")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        documents = []
        total_texts = len(texts)
        for i, text in enumerate(texts):
            splits = text_splitter.split_text(text)
            for split in splits:
                documents.append(LangDocument(
                    page_content=split,
                    metadata={"source": list(docs_dict.keys())[i % len(docs_dict)]}
                ))
            
            if progress_callback and total_texts > 0:
                progress = 30 + int((i + 1) / total_texts * 20)
                progress_callback(progress, f"ğŸ”„ æ­¥éª¤ 2/4: åˆ†å‰²æ–‡æœ¬... ({i + 1}/{total_texts})")
        
        # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
        if progress_callback:
            progress_callback(55, "ğŸ”„ æ­¥éª¤ 3/4: åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œé¿å…ç½‘ç»œä¸‹è½½
        model_path = get_model_path("BAAI/bge-small-zh-v1.5")
        
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=model_path,  # ä¸­æ–‡ä¼˜åŒ–çš„å°æ¨¡å‹
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as model_error:
            error_type = type(model_error).__name__
            error_msg = str(model_error)
            raise Exception(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ [{error_type}]: {error_msg}\n\n"
                          f"å¯èƒ½çš„åŸå› :\n"
                          f"- æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–æŸå\n"
                          f"- ç½‘ç»œè¿æ¥é—®é¢˜ï¼ˆæ— æ³•ä¸‹è½½æ¨¡å‹ï¼‰\n"
                          f"- æ¨¡å‹è·¯å¾„é…ç½®é”™è¯¯: {model_path}\n\n"
                          f"è§£å†³æ–¹æ¡ˆ:\n"
                          f"- æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n"
                          f"- ä½¿ç”¨ download_model.py æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹\n"
                          f"- æ£€æŸ¥ç½‘ç»œè¿æ¥") from model_error
        
        if progress_callback:
            progress_callback(70, "ğŸ”„ æ­¥éª¤ 3/4: ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸ºç©º
        if not documents or len(documents) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•åˆ›å»ºå‘é‡æ•°æ®åº“ã€‚è¯·æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸ºç©ºæˆ–æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        if progress_callback:
            progress_callback(85, "ğŸ”„ æ­¥éª¤ 4/4: åˆ›å»ºå‘é‡å­˜å‚¨...")
        
        # ç¡®ä¿ä½¿ç”¨å…¨æ–°çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä»ç„¶å­˜åœ¨ï¼Œå†æ¬¡æ¸…ç†ï¼‰
        if os.path.exists(db_path):
            # æœ€åä¸€æ¬¡æ¸…ç†å°è¯•
            cleanup_corrupted_db(db_path, force=True)
            import time
            time.sleep(0.5)
        
        # ç¡®ä¿ç›®å½•ä¸å­˜åœ¨åå†åˆ›å»º
        if not os.path.exists(db_path):
            os.makedirs(db_path, exist_ok=True)
        else:
            # å¦‚æœä»ç„¶å­˜åœ¨ï¼Œå°è¯•é‡å‘½å
            import time
            backup_name = db_path + "_backup_" + str(int(time.time()))
            try:
                os.rename(db_path, backup_name)
                print(f"âš ï¸ æ— æ³•åˆ é™¤ç›®å½•ï¼Œå·²é‡å‘½åä¸ºå¤‡ä»½: {backup_name}")
                os.makedirs(db_path, exist_ok=True)
            except Exception:
                # å¦‚æœé‡å‘½åä¹Ÿå¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤
                cleanup_corrupted_db(db_path, force=True)
                time.sleep(0.5)
                if not os.path.exists(db_path):
                    os.makedirs(db_path, exist_ok=True)
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„å‚æ•°å
        max_retries = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # æ–°ç‰ˆæœ¬ä½¿ç”¨ embedding_function
                if progress_callback:
                    progress_callback(88, f"ğŸ”„ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰...")
                vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding_function=embeddings,
                    persist_directory=db_path
                )
                break  # æˆåŠŸåˆ›å»ºï¼Œé€€å‡ºå¾ªç¯
            except TypeError as type_error:
                # æ—§ç‰ˆæœ¬ä½¿ç”¨ embedding
                if progress_callback:
                    progress_callback(88, "ğŸ”„ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼‰...")
                try:
                    vectorstore = Chroma.from_documents(
                        documents=documents,
                        embedding=embeddings,
                        persist_directory=db_path
                    )
                    break  # æˆåŠŸåˆ›å»ºï¼Œé€€å‡ºå¾ªç¯
                except Exception as e:
                    last_error = e
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”™è¯¯ï¼ˆåŒ…æ‹¬ HNSW ç´¢å¼•é”™è¯¯å’Œ schema é”™è¯¯ï¼‰
                    error_msg = str(e).lower()
                    is_db_error = ("hnsw" in error_msg or "index" in error_msg or "compaction" in error_msg or 
                                  "segment" in error_msg or "no such column" in error_msg or 
                                  "collections.topic" in error_msg or "schema" in error_msg)
                    
                    if is_db_error and attempt < max_retries - 1:
                        # å¦‚æœæ˜¯æ•°æ®åº“é”™è¯¯ï¼Œå¼ºåˆ¶æ¸…ç†æ•°æ®åº“å¹¶é‡è¯•
                        if progress_callback:
                            progress_callback(87, "ğŸ”„ æ£€æµ‹åˆ°æ•°æ®åº“é”™è¯¯ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œå¼ºåˆ¶æ¸…ç†å¹¶é‡è¯•...")
                        import time
                        cleanup_corrupted_db(db_path, force=True)
                        time.sleep(1)  # ç­‰å¾…æ–‡ä»¶ç³»ç»Ÿå®Œå…¨é‡Šæ”¾
                        if not os.path.exists(db_path):
                            os.makedirs(db_path, exist_ok=True)
                        continue
                    elif is_db_error:
                        # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œä½†ä»ç„¶æŠ›å‡ºè¯¦ç»†é”™è¯¯
                        raise Exception(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥ï¼ˆå…¼å®¹æ¨¡å¼ï¼Œå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {str(e)}\n\n"
                                      f"æ£€æµ‹åˆ°æ•°æ®åº“é”™è¯¯ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œå·²å°è¯•æ¸…ç†å¹¶é‡è¯•ï¼Œä½†ä»ç„¶å¤±è´¥ã€‚\n"
                                      f"é”™è¯¯ç±»å‹: {error_msg}\n\n"
                                      f"è§£å†³æ–¹æ¡ˆ:\n"
                                      f"1. æ‰‹åŠ¨åˆ é™¤æ•°æ®åº“ç›®å½•: {db_path}\n"
                                      f"2. æ£€æŸ¥ ChromaDB ç‰ˆæœ¬å…¼å®¹æ€§\n"
                                      f"3. å°è¯•é™çº§ ChromaDB: poetry add chromadb==0.4.22") from e
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                        raise Exception(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰: {str(e)}") from e
            except Exception as create_error:
                last_error = create_error
                error_msg = str(create_error).lower()
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”™è¯¯ï¼ˆåŒ…æ‹¬ HNSW ç´¢å¼•é”™è¯¯å’Œ schema é”™è¯¯ï¼‰
                is_db_error = ("hnsw" in error_msg or "index" in error_msg or "compaction" in error_msg or 
                              "segment" in error_msg or "no such column" in error_msg or 
                              "collections.topic" in error_msg or "schema" in error_msg)
                
                if is_db_error and attempt < max_retries - 1:
                    # å¦‚æœæ˜¯æ•°æ®åº“é”™è¯¯ï¼Œå¼ºåˆ¶æ¸…ç†æ•°æ®åº“å¹¶é‡è¯•
                    if progress_callback:
                        progress_callback(87, "ğŸ”„ æ£€æµ‹åˆ°æ•°æ®åº“é”™è¯¯ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œå¼ºåˆ¶æ¸…ç†å¹¶é‡è¯•...")
                    import time
                    cleanup_corrupted_db(db_path, force=True)
                    time.sleep(1)  # ç­‰å¾…æ–‡ä»¶ç³»ç»Ÿå®Œå…¨é‡Šæ”¾
                    if not os.path.exists(db_path):
                        os.makedirs(db_path, exist_ok=True)
                    continue
                elif is_db_error:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œä½†ä»ç„¶æŠ›å‡ºè¯¦ç»†é”™è¯¯
                    error_type = type(create_error).__name__
                    raise Exception(f"åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™ [{error_type}]ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {str(create_error)}\n\n"
                                  f"æ£€æµ‹åˆ°æ•°æ®åº“é”™è¯¯ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ï¼Œå·²å°è¯•æ¸…ç†å¹¶é‡è¯•ï¼Œä½†ä»ç„¶å¤±è´¥ã€‚\n"
                                  f"é”™è¯¯ç±»å‹: {error_msg}\n\n"
                                  f"è§£å†³æ–¹æ¡ˆ:\n"
                                  f"1. æ‰‹åŠ¨åˆ é™¤æ•°æ®åº“ç›®å½•: {db_path}\n"
                                  f"2. æ£€æŸ¥ ChromaDB ç‰ˆæœ¬å…¼å®¹æ€§\n"
                                  f"3. å°è¯•é™çº§ ChromaDB: poetry add chromadb==0.4.22") from create_error
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    error_type = type(create_error).__name__
                    raise Exception(f"åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™ [{error_type}]: {str(create_error)}") from create_error
        else:
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            if last_error:
                error_type = type(last_error).__name__
                raise Exception(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ [{error_type}]: {str(last_error)}") from last_error
            else:
                raise Exception("åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯")
        
        if progress_callback:
            progress_callback(100, "âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
        
        # ä¿å­˜æ–‡æ¡£ç­¾å
        if folder_path:
            save_docs_signature(docs_dict, folder_path)
        
        return vectorstore
    except ImportError as e:
        # å¯¼å…¥é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç¼ºå°‘ä¾èµ–åŒ…
        error_msg = str(e)
        if 'st' in globals():
            st.warning(f"âš ï¸ å‘é‡æ•°æ®åº“åŠŸèƒ½ä¸å¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–åŒ…ï¼‰: {error_msg}\n\nğŸ’¡ æç¤ºï¼š\n- å‘é‡æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨\n- æ–‡æ¡£é˜…è¯»å’Œé—®ç­”åŠŸèƒ½ä»å¯æ­£å¸¸ä½¿ç”¨\n- å¦‚éœ€ä½¿ç”¨å‘é‡æœç´¢ï¼Œè¯·å®‰è£…: pip install langchain-text-splitters langchain-community chromadb")
        return None
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼Œæ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        error_type = type(e).__name__
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ NumPy 2.0 å…¼å®¹æ€§é—®é¢˜
        is_numpy_error = ("np.float_" in error_msg or "numpy" in error_msg.lower() or 
                         "AttributeError" in error_type and "float_" in error_msg)
        
        # è®°å½•é”™è¯¯åˆ°æ§åˆ¶å°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        import traceback
        print(f"âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥:")
        print(f"   é”™è¯¯ç±»å‹: {error_type}")
        print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
        print(f"   è¯¦ç»†å †æ ˆ:")
        traceback.print_exc()
        
        if 'st' in globals():
            if is_numpy_error:
                # NumPy 2.0 å…¼å®¹æ€§é”™è¯¯
                st.error(f"âš ï¸ **å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥ - NumPy ç‰ˆæœ¬ä¸å…¼å®¹**\n\n"
                        f"**é”™è¯¯ç±»å‹**: `{error_type}`\n\n"
                        f"**é”™è¯¯ä¿¡æ¯**: {error_msg}\n\n"
                        f"**é—®é¢˜åŸå› **:\n"
                        f"ChromaDB ä¸å…¼å®¹ NumPy 2.0ï¼Œå½“å‰ç¯å¢ƒå¯èƒ½å®‰è£…äº† NumPy 2.0\n\n"
                        f"**è§£å†³æ–¹æ¡ˆ**:\n"
                        f"1. **é™çº§ NumPy åˆ° 1.x ç‰ˆæœ¬**ï¼ˆæ¨èï¼‰:\n"
                        f"   ```bash\n"
                        f"   poetry remove numpy\n"
                        f"   poetry add \"numpy>=1.24.0,<2.0.0\"\n"
                        f"   ```\n"
                        f"   æˆ–ä½¿ç”¨ pip:\n"
                        f"   ```bash\n"
                        f"   pip install \"numpy>=1.24.0,<2.0.0\"\n"
                        f"   ```\n\n"
                        f"2. **é‡æ–°å®‰è£…æ‰€æœ‰ä¾èµ–**:\n"
                        f"   ```bash\n"
                        f"   poetry install\n"
                        f"   ```\n\n"
                        f"3. **æ£€æŸ¥ NumPy ç‰ˆæœ¬**:\n"
                        f"   ```bash\n"
                        f"   poetry run python -c \"import numpy; print(numpy.__version__)\"\n"
                        f"   ```\n\n"
                        f"ğŸ’¡ **æç¤º**: ä¿®å¤åé‡æ–°è¿è¡Œç¨‹åºå³å¯")
            else:
                # å…¶ä»–é”™è¯¯
                st.error(f"âš ï¸ **å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥**\n\n"
                        f"**é”™è¯¯ç±»å‹**: `{error_type}`\n\n"
                        f"**é”™è¯¯ä¿¡æ¯**: {error_msg}\n\n"
                        f"**å¯èƒ½çš„åŸå› **:\n"
                        f"- æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®\n"
                        f"- åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹æ–‡ä»¶ï¼‰\n"
                        f"- ChromaDB ç‰ˆæœ¬ä¸å…¼å®¹\n"
                        f"- NumPy ç‰ˆæœ¬ä¸å…¼å®¹ï¼ˆNumPy 2.0 ä¸å…¼å®¹ï¼‰\n"
                        f"- ç£ç›˜ç©ºé—´ä¸è¶³æˆ–æ²¡æœ‰å†™å…¥æƒé™\n"
                        f"- å†…å­˜ä¸è¶³ï¼ˆæ–‡æ¡£å¤ªå¤§ï¼‰\n\n"
                        f"**è§£å†³æ–¹æ¡ˆ**:\n"
                        f"1. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸ºç©º\n"
                        f"2. æ£€æŸ¥æ§åˆ¶å°è¾“å‡ºçš„è¯¦ç»†é”™è¯¯ä¿¡æ¯\n"
                        f"3. å°è¯•å‡å°‘æ–‡æ¡£æ•°é‡æˆ–å¤§å°\n"
                        f"4. æ£€æŸ¥ç£ç›˜ç©ºé—´å’Œæƒé™\n"
                        f"5. å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æŸ¥çœ‹å®Œæ•´é”™è¯¯å †æ ˆ\n\n"
                        f"ğŸ’¡ **æç¤º**: å‘é‡æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†æ–‡æ¡£é˜…è¯»å’Œé—®ç­”åŠŸèƒ½ä»å¯æ­£å¸¸ä½¿ç”¨ï¼ˆä¼šä½¿ç”¨æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œå¯èƒ½è¾ƒæ…¢ï¼‰")
        return None

# DeepSeek APIæ¥å£
def query_deepseek(prompt: str, api_key: str, model: str = "deepseek-chat", max_tokens: int = 2000, 
                   max_retries: int = 3, timeout: int = None):
    """è°ƒç”¨DeepSeek APIï¼Œå¸¦é‡è¯•æœºåˆ¶
    
    Args:
        prompt: æç¤ºæ–‡æœ¬
        api_key: DeepSeek APIå¯†é’¥
        model: æ¨¡å‹åç§°
        max_tokens: æœ€å¤§tokenæ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼æˆ–ä»session_stateè·å–
    """
    import requests
    import time
    
    # ä» session_state è·å–è¶…æ—¶å’Œé‡è¯•é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if timeout is None:
        if 'st' in globals() and hasattr(st, 'session_state'):
            timeout = st.session_state.get('api_timeout', 60)
        else:
            timeout = 60
    
    if max_retries is None or max_retries == 3:
        if 'st' in globals() and hasattr(st, 'session_state'):
            max_retries = st.session_state.get('api_max_retries', 3)
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # æ ¹æ®å°è¯•æ¬¡æ•°å¢åŠ è¶…æ—¶æ—¶é—´
            current_timeout = timeout + (attempt * 20)
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=current_timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    return result["choices"][0]["message"]["content"]
                else:
                    return "APIè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè¯·é‡è¯•"
            elif response.status_code == 401:
                return "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„DeepSeek APIå¯†é’¥"
            elif response.status_code == 429:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€8ç§’
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                    continue
                return "APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•"
            elif response.status_code == 500:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return "DeepSeekæœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"
            else:
                return f"APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text[:200]}"
                
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                error_msg = f"è¯·æ±‚è¶…æ—¶ï¼ˆå·²å°è¯• {attempt + 1}/{max_retries} æ¬¡ï¼‰ï¼Œ{wait_time}ç§’åé‡è¯•..."
                if 'st' in globals():
                    st.warning(error_msg)
                time.sleep(wait_time)
                continue
            else:
                return f"è¯·æ±‚è¶…æ—¶ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ã€‚å¯èƒ½çš„åŸå› ï¼š\n1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n2. è¯·æ±‚å†…å®¹è¿‡é•¿\n3. DeepSeekæœåŠ¡å™¨å“åº”æ…¢\n\nå»ºè®®ï¼š\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- å°è¯•å‡å°‘æ–‡æ¡£å†…å®¹\n- ç¨åé‡è¯•"
        
        except requests.exceptions.ConnectionError:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                error_msg = f"è¿æ¥é”™è¯¯ï¼ˆå·²å°è¯• {attempt + 1}/{max_retries} æ¬¡ï¼‰ï¼Œ{wait_time}ç§’åé‡è¯•..."
                if 'st' in globals():
                    st.warning(error_msg)
                time.sleep(wait_time)
                continue
            else:
                return "æ— æ³•è¿æ¥åˆ°DeepSeek APIæœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ï¼š\n1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n2. æ˜¯å¦å¯ä»¥ä½¿ç”¨ä»£ç†è®¿é—®\n3. DeepSeekæœåŠ¡æ˜¯å¦æ­£å¸¸"
        
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            return f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}"
        
        except Exception as e:
            return f"è°ƒç”¨APIæ—¶å‡ºé”™: {str(e)}"
    
    return "APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•å¤šæ¬¡ä»æ— æ³•æˆåŠŸ"

def search_similar_documents(vectorstore, query: str, k: int = 4):
    """æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ç‰‡æ®µ"""
    if vectorstore is None:
        return []
    
    try:
        docs = vectorstore.similarity_search(query, k=k)
        return [(doc.page_content, doc.metadata["source"]) for doc in docs]
    except:
        return []

def answer_with_deepseek(question: str, vectorstore, docs_dict: Dict[str, Any], api_key: str):
    """ä½¿ç”¨DeepSeekå›ç­”é—®é¢˜"""
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    similar_docs = search_similar_documents(vectorstore, question)
    
    if not similar_docs:
        # å¦‚æœæ²¡æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨æ‰€æœ‰æ–‡æ¡£å†…å®¹
        context = "\n\n".join([f"æ–‡ä»¶: {name}\nå†…å®¹: {data['content'][:2000]}..." 
                             for name, data in docs_dict.items()])
    else:
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
        context_parts = []
        for content, source in similar_docs:
            context_parts.append(f"æ¥è‡ªæ–‡æ¡£ '{source}' çš„å†…å®¹:\n{content}")
        context = "\n\n".join(context_parts)
    
    # æ„å»ºæç¤º
    prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œè¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{question}

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context[:8000]}  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""

    return query_deepseek(prompt, api_key)

def generate_summary_deepseek(docs_dict: Dict[str, Any], api_key: str, specific_files: List[str] = None):
    """ä½¿ç”¨DeepSeekç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    # æå–å†…å®¹
    contents = []
    if specific_files:
        for filename in specific_files:
            if filename in docs_dict:
                content = docs_dict[filename]['content']
                if isinstance(content, dict):
                    content = "\n".join([f"{k}: {v[:1000]}" for k, v in content.items()])
                contents.append(f"æ–‡ä»¶: {filename}\n{content}")
    else:
        for filename, data in docs_dict.items():
            content = data['content']
            if isinstance(content, dict):
                content = "\n".join([f"{k}: {v[:1000]}" for k, v in content.items()])
            contents.append(f"æ–‡ä»¶: {filename}\n{content}")
    
    combined_content = "\n\n".join(contents)
    
    # æ„å»ºæç¤º
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ€»ç»“æŠ¥å‘Šï¼š

æ–‡æ¡£å†…å®¹ï¼š
{combined_content[:12000]}

è¯·ç”ŸæˆåŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†çš„æŠ¥å‘Šï¼š
1. æ•´ä½“å†…å®¹æ¦‚è¿°
2. æ ¸å¿ƒè¦ç‚¹æ€»ç»“
3. å…³é”®æ•°æ®/ä¿¡æ¯æå–
4. ä¸»è¦å‘ç°å’Œæ´å¯Ÿ
5. å»ºè®®å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨

æŠ¥å‘Šï¼š"""

    return query_deepseek(prompt, api_key, max_tokens=3000)

# æ˜¾ç¤ºç‰ˆæƒä¿¡æ¯
def show_footer():
    """åœ¨é¡µé¢åº•éƒ¨æ˜¾ç¤ºç‰ˆæƒä¿¡æ¯"""
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 20px 0;'>
            <p style='margin: 5px 0;'><strong>Copyright Â© 2026 å•æ»¢</strong></p>
            <p style='margin: 5px 0;'>
                GitHub: <a href='https://github.com/lveyond' target='_blank' style='color: #1f77b4;'>@lveyond</a> | 
                QQ/WeChat: 329613507
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )

# Streamlitç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ (DeepSeekç‰ˆ)",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    st.title("ğŸ“š æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ (DeepSeekç‰ˆ)")
    st.markdown("---")
    
    # åˆå§‹åŒ–session state
    if 'docs' not in st.session_state:
        st.session_state.docs = {}
    if 'vectorstore' not in st.session_state:
        st.session_state.vectorstore = None
    if 'selected_file' not in st.session_state:
        st.session_state.selected_file = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'api_key_loaded' not in st.session_state:
        st.session_state.api_key_loaded = False
    if 'is_creating_vectorstore' not in st.session_state:
        st.session_state.is_creating_vectorstore = False
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        
        # DeepSeek APIé…ç½®
        st.subheader("ğŸ”‘ DeepSeek APIè®¾ç½®")
        
        # å°è¯•ä»æœ¬åœ°åŠ è½½ API key
        saved_api_key = None
        if not st.session_state.api_key_loaded:
            saved_api_key = load_api_key()
            if saved_api_key:
                st.session_state.api_key_loaded = True
                st.session_state.saved_api_key = saved_api_key
                st.success("âœ… å·²ä»æœ¬åœ°åŠ è½½ API å¯†é’¥")
        
        # ä½¿ç”¨ä¿å­˜çš„ key æˆ–ç”¨æˆ·è¾“å…¥
        default_key = st.session_state.get('saved_api_key', '') if st.session_state.api_key_loaded else ''
        api_key_input = st.text_input(
            "DeepSeek APIå¯†é’¥", 
            value=default_key,
            type="password", 
            help="åœ¨ https://platform.deepseek.com/ è·å–APIå¯†é’¥\nğŸ’¾ å¯†é’¥ä¼šè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡å¯åŠ¨æ— éœ€é‡æ–°è¾“å…¥"
        )
        
        # API key ç®¡ç†æŒ‰é’®
        col_key1, col_key2 = st.columns(2)
        with col_key1:
            if st.button("ğŸ’¾ ä¿å­˜å¯†é’¥", use_container_width=True):
                if api_key_input:
                    if save_api_key(api_key_input):
                        st.session_state.saved_api_key = api_key_input
                        st.session_state.api_key_loaded = True
                        st.success("âœ… API å¯†é’¥å·²ä¿å­˜åˆ°æœ¬åœ°")
                        st.rerun()
                else:
                    st.warning("è¯·è¾“å…¥ API å¯†é’¥")
        
        with col_key2:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯†é’¥", use_container_width=True):
                if delete_api_key():
                    st.session_state.saved_api_key = ""
                    st.session_state.api_key_loaded = False
                    st.success("âœ… å·²æ¸…é™¤æœ¬åœ°ä¿å­˜çš„ API å¯†é’¥")
                    st.rerun()
        
        # è‡ªåŠ¨ä¿å­˜é€»è¾‘ï¼šå¦‚æœç”¨æˆ·è¾“å…¥äº†æ–°å¯†é’¥ä¸”ä¸ä¿å­˜çš„ä¸åŒï¼Œè‡ªåŠ¨ä¿å­˜
        if api_key_input and api_key_input != st.session_state.get('saved_api_key', ''):
            # ç”¨æˆ·è¾“å…¥äº†æ–°å¯†é’¥ï¼Œè‡ªåŠ¨ä¿å­˜ï¼ˆä»…åœ¨é¦–æ¬¡è¾“å…¥æ—¶ï¼Œé™é»˜ä¿å­˜ï¼‰
            if not st.session_state.api_key_loaded or api_key_input != saved_api_key:
                if save_api_key(api_key_input, show_error=False):
                    st.session_state.saved_api_key = api_key_input
                    st.session_state.api_key_loaded = True
                    # é™é»˜ä¿å­˜ï¼Œä¸æ˜¾ç¤ºæç¤ºï¼ˆé¿å…é¢‘ç¹åˆ·æ–°ï¼‰
        
        # ä½¿ç”¨è¾“å…¥çš„ keyï¼ˆä¼˜å…ˆä½¿ç”¨æ–°è¾“å…¥çš„ï¼‰
        api_key = api_key_input if api_key_input else (st.session_state.get('saved_api_key', '') if st.session_state.api_key_loaded else '')
        
        # æ˜¾ç¤ºä¿å­˜çŠ¶æ€
        if os.path.exists(CONFIG_FILE):
            saved_time = ""
            try:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    saved_time = config.get("saved_at", "")
            except:
                pass
            if saved_time:
                st.caption(f"ğŸ’¾ å¯†é’¥å·²ä¿å­˜ï¼ˆ{saved_time}ï¼‰")
            else:
                st.caption("ğŸ’¾ å¯†é’¥å·²ä¿å­˜åˆ°æœ¬åœ°")
        else:
            st.caption("ğŸ’¡ è¾“å…¥å¯†é’¥åä¼šè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°")
        
        # æ¨¡å‹é€‰æ‹©
        model_choice = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            ["deepseek-chat", "deepseek-coder"],
            help="deepseek-chat: é€šç”¨å¯¹è¯æ¨¡å‹\ndeepseek-coder: ä»£ç ä¸“ç”¨æ¨¡å‹"
        )
        
        # API è¶…æ—¶å’Œé‡è¯•é…ç½®
        if 'api_timeout' not in st.session_state:
            st.session_state.api_timeout = 60
        if 'api_max_retries' not in st.session_state:
            st.session_state.api_max_retries = 3
        
        with st.expander("âš™ï¸ é«˜çº§è®¾ç½®ï¼ˆç½‘ç»œé—®é¢˜æ—¶å¯è°ƒæ•´ï¼‰", expanded=False):
            timeout_seconds = st.slider(
                "è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
                min_value=30,
                max_value=180,
                value=st.session_state.api_timeout,
                step=10,
                help="å¦‚æœç»å¸¸è¶…æ—¶ï¼Œå¯ä»¥å¢åŠ æ­¤å€¼"
            )
            max_retries = st.slider(
                "æœ€å¤§é‡è¯•æ¬¡æ•°",
                min_value=1,
                max_value=5,
                value=st.session_state.api_max_retries,
                step=1,
                help="ç½‘ç»œä¸ç¨³å®šæ—¶å¯ä»¥å¢åŠ é‡è¯•æ¬¡æ•°"
            )
            
            # ä¿å­˜åˆ° session state
            st.session_state.api_timeout = timeout_seconds
            st.session_state.api_max_retries = max_retries
        
        st.markdown("---")
        st.header("ğŸ“ æ–‡ä»¶ç®¡ç†")
        
        # æ–‡ä»¶å¤¹é€‰æ‹©
        folder_path = st.text_input("æ–‡ä»¶å¤¹è·¯å¾„", placeholder="è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚: ./documents")
        
        col1, col2 = st.columns(2)
        with col1:
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“
            is_creating_vectorstore = st.session_state.get('is_creating_vectorstore', False)
            
            if st.button("ğŸ“‚ åŠ è½½æ–‡ä»¶å¤¹", use_container_width=True, disabled=is_creating_vectorstore):
                if folder_path and os.path.exists(folder_path) and os.path.isdir(folder_path):
                    with st.spinner("æ­£åœ¨è¯»å–æ–‡ä»¶..."):
                        st.session_state.docs = process_folder(folder_path)
                        # ä¿å­˜å½“å‰æ–‡ä»¶å¤¹è·¯å¾„
                        st.session_state.current_folder_path = folder_path
                    st.success(f"å·²åŠ è½½ {len(st.session_state.docs)} ä¸ªæ–‡ä»¶")
                    
                    # æ£€æŸ¥å¹¶åŠ è½½/åˆ›å»ºå‘é‡æ•°æ®åº“
                    if st.session_state.docs:
                        st.session_state.is_creating_vectorstore = True
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        try:
                            # é¦–å…ˆå°è¯•åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“
                            status_text.text("ğŸ”„ æ­£åœ¨æ£€æŸ¥å·²æœ‰å‘é‡æ•°æ®åº“...")
                            progress_bar.progress(5)
                            
                            existing_vectorstore = load_existing_vector_store(
                                folder_path=folder_path,
                                progress_callback=lambda p, msg: (
                                    progress_bar.progress(p / 100.0),
                                    status_text.text(msg)
                                )
                            )
                            
                            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å˜åŒ–
                            docs_changed = check_docs_changed(st.session_state.docs, folder_path)
                            
                            # å®‰å…¨åœ°æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å¯ç”¨ï¼ˆé¿å…ç´¢å¼•æŸåå¯¼è‡´çš„é”™è¯¯ï¼‰
                            vectorstore_usable = False
                            if existing_vectorstore:
                                try:
                                    # å°è¯•è®¿é—®å‘é‡æ•°æ®åº“ï¼Œå¦‚æœç´¢å¼•æŸåä¼šåœ¨è¿™é‡Œå¤±è´¥
                                    _ = len(existing_vectorstore)
                                    vectorstore_usable = True
                                except Exception as e:
                                    # å‘é‡æ•°æ®åº“ç´¢å¼•å¯èƒ½æŸåï¼Œéœ€è¦é‡æ–°åˆ›å»º
                                    vectorstore_usable = False
                                    status_text.text("âš ï¸ æ£€æµ‹åˆ°å‘é‡æ•°æ®åº“ç´¢å¼•æŸåï¼Œå°†é‡æ–°åˆ›å»º...")
                                    progress_bar.progress(0.1)
                            
                            if vectorstore_usable and not docs_changed:
                                # æ–‡æ¡£æœªå˜åŒ–ï¼Œä½¿ç”¨å·²æœ‰å‘é‡æ•°æ®åº“
                                st.session_state.vectorstore = existing_vectorstore
                                progress_bar.progress(100)
                                status_text.text("âœ… å·²åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“ï¼")
                                st.success("âœ… å·²åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“ï¼ˆæ–‡æ¡£æœªå˜åŒ–ï¼‰")
                            else:
                                # æ–‡æ¡£å˜åŒ–æˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°åˆ›å»º
                                if existing_vectorstore and docs_changed:
                                    status_text.text("ğŸ“ æ£€æµ‹åˆ°æ–‡æ¡£å˜åŒ–ï¼Œæ­£åœ¨é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
                                    progress_bar.progress(10)
                                
                                status_text.text("ğŸ”„ æ­¥éª¤ 1/4: å‡†å¤‡æ–‡æœ¬å†…å®¹...")
                                progress_bar.progress(10)
                                
                                status_text.text("ğŸ”„ æ­¥éª¤ 2/4: åˆ†å‰²æ–‡æœ¬...")
                                progress_bar.progress(30)
                                
                                status_text.text("ğŸ”„ æ­¥éª¤ 3/4: ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                                progress_bar.progress(50)
                                
                                st.session_state.vectorstore = create_local_vector_store(
                                    st.session_state.docs,
                                    folder_path=folder_path,
                                    progress_callback=lambda p, msg: (
                                        progress_bar.progress(p / 100.0),
                                        status_text.text(msg)
                                    )
                                )
                                
                                progress_bar.progress(90)
                                status_text.text("ğŸ”„ æ­¥éª¤ 4/4: ä¿å­˜å‘é‡æ•°æ®åº“...")
                                
                                if st.session_state.vectorstore:
                                    progress_bar.progress(100)
                                    status_text.text("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
                                    st.success("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
                                else:
                                    progress_bar.empty()
                                    status_text.empty()
                                    # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œå·²ç»åœ¨ create_local_vector_store ä¸­æ˜¾ç¤ºäº†è­¦å‘Š
                        finally:
                            st.session_state.is_creating_vectorstore = False
                            # æ¸…ç†è¿›åº¦æ¡
                            import time
                            time.sleep(0.5)
                            progress_bar.empty()
                            status_text.empty()
                else:
                    st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„")
        
        with col2:
            if st.button("ğŸ”„ é‡æ–°åŠ è½½", use_container_width=True, disabled=is_creating_vectorstore):
                st.session_state.docs = {}
                st.session_state.vectorstore = None
                st.session_state.is_creating_vectorstore = False
                st.rerun()
        
        # æ–‡ä»¶ä¸Šä¼ 
        st.subheader("æˆ–ä¸Šä¼ æ–‡ä»¶")
        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            type=['txt', 'docx', 'pdf', 'xlsx', 'xls'],
            accept_multiple_files=True,
            label_visibility="collapsed"
        )
        
        if uploaded_files and st.button("ä¸Šä¼ æ–‡ä»¶"):
            temp_dir = tempfile.mkdtemp()
            for uploaded_file in uploaded_files:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # è¯»å–æ–‡ä»¶
                filename = uploaded_file.name
                file_ext = filename.split('.')[-1].lower()
                
                try:
                    if file_ext == 'txt':
                        content = read_text_file(file_path)
                    elif file_ext == 'docx':
                        content = read_docx_file(file_path)
                    elif file_ext == 'pdf':
                        content = read_pdf_file(file_path)
                    elif file_ext in ['xlsx', 'xls']:
                        content = read_excel_file(file_path)
                    else:
                        content = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
                    
                    st.session_state.docs[filename] = {
                        'path': file_path,
                        'content': content,
                        'type': file_ext,
                        'size': uploaded_file.size
                    }
                except Exception as e:
                    st.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
            
            st.success(f"å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            
            # æ£€æŸ¥å¹¶åŠ è½½/åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆä¸Šä¼ æ–‡ä»¶æ—¶ folder_path ä¸º Noneï¼‰
            if st.session_state.docs:
                st.session_state.is_creating_vectorstore = True
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    # é¦–å…ˆå°è¯•åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“
                    status_text.text("ğŸ”„ æ­£åœ¨æ£€æŸ¥å·²æœ‰å‘é‡æ•°æ®åº“...")
                    progress_bar.progress(5)
                    
                    existing_vectorstore = load_existing_vector_store(
                        folder_path=None,  # ä¸Šä¼ æ–‡ä»¶æ—¶æ²¡æœ‰æ–‡ä»¶å¤¹è·¯å¾„
                        progress_callback=lambda p, msg: (
                            progress_bar.progress(p / 100.0),
                            status_text.text(msg)
                        )
                    )
                    
                    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å˜åŒ–
                    docs_changed = check_docs_changed(st.session_state.docs, None)
                    
                    # å®‰å…¨åœ°æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å¯ç”¨
                    vectorstore_usable = False
                    if existing_vectorstore:
                        try:
                            _ = len(existing_vectorstore)
                            vectorstore_usable = True
                        except Exception as e:
                            vectorstore_usable = False
                            status_text.text("âš ï¸ æ£€æµ‹åˆ°å‘é‡æ•°æ®åº“ç´¢å¼•æŸåï¼Œå°†é‡æ–°åˆ›å»º...")
                            progress_bar.progress(0.1)
                    
                    if vectorstore_usable and not docs_changed:
                        # æ–‡æ¡£æœªå˜åŒ–ï¼Œä½¿ç”¨å·²æœ‰å‘é‡æ•°æ®åº“
                        st.session_state.vectorstore = existing_vectorstore
                        progress_bar.progress(100)
                        status_text.text("âœ… å·²åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“ï¼")
                        st.success("âœ… å·²åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“ï¼ˆæ–‡æ¡£æœªå˜åŒ–ï¼‰")
                    else:
                        # æ–‡æ¡£å˜åŒ–æˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°åˆ›å»º
                        if existing_vectorstore and docs_changed:
                            status_text.text("ğŸ“ æ£€æµ‹åˆ°æ–‡æ¡£å˜åŒ–ï¼Œæ­£åœ¨é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
                            progress_bar.progress(10)
                        
                        status_text.text("ğŸ”„ æ­¥éª¤ 1/4: å‡†å¤‡æ–‡æœ¬å†…å®¹...")
                        progress_bar.progress(10)
                        
                        status_text.text("ğŸ”„ æ­¥éª¤ 2/4: åˆ†å‰²æ–‡æœ¬...")
                        progress_bar.progress(30)
                        
                        status_text.text("ğŸ”„ æ­¥éª¤ 3/4: ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                        progress_bar.progress(50)
                        
                        st.session_state.vectorstore = create_local_vector_store(
                            st.session_state.docs,
                            folder_path=None,  # ä¸Šä¼ æ–‡ä»¶æ—¶æ²¡æœ‰æ–‡ä»¶å¤¹è·¯å¾„
                            progress_callback=lambda p, msg: (
                                progress_bar.progress(p / 100.0),
                                status_text.text(msg)
                            )
                        )
                        
                        progress_bar.progress(90)
                        status_text.text("ğŸ”„ æ­¥éª¤ 4/4: ä¿å­˜å‘é‡æ•°æ®åº“...")
                    
                    if st.session_state.vectorstore:
                        progress_bar.progress(100)
                        status_text.text("âœ… å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆï¼")
                        st.success("âœ… å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆï¼")
                    else:
                        progress_bar.empty()
                        status_text.empty()
                finally:
                    st.session_state.is_creating_vectorstore = False
                    import time
                    time.sleep(0.5)
                    progress_bar.empty()
                    status_text.empty()
        
        # æ–‡ä»¶ç»Ÿè®¡
        if st.session_state.docs:
            st.markdown("---")
            st.header("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
            total_files = len(st.session_state.docs)
            file_types = {}
            for data in st.session_state.docs.values():
                file_type = data['type']
                file_types[file_type] = file_types.get(file_type, 0) + 1
            
            st.write(f"**æ–‡ä»¶æ€»æ•°**: {total_files}")
            for ftype, count in file_types.items():
                st.write(f"**{ftype}æ–‡ä»¶**: {count}ä¸ª")
        
        st.markdown("---")
        
        # å‘é‡æ•°æ®åº“ç®¡ç†
        if st.session_state.docs:
            with st.expander("ğŸ—‘ï¸ å‘é‡æ•°æ®åº“ç®¡ç†", expanded=False):
                col_vdb1, col_vdb2 = st.columns(2)
                
                with col_vdb1:
                    # è·å–å½“å‰æ–‡ä»¶å¤¹çš„å‘é‡æ•°æ®åº“è·¯å¾„
                    current_folder_path = st.session_state.get('current_folder_path', None)
                    if current_folder_path:
                        current_db_path = get_vector_db_path(current_folder_path)
                        if st.button("ğŸ—‘ï¸ åˆ é™¤å½“å‰å‘é‡æ•°æ®åº“", use_container_width=True):
                            import shutil
                            if os.path.exists(current_db_path):
                                try:
                                    shutil.rmtree(current_db_path)
                                    st.session_state.vectorstore = None
                                    st.success("âœ… å½“å‰å‘é‡æ•°æ®åº“å·²åˆ é™¤")
                                    st.info("ğŸ’¡ ä¸‹æ¬¡åŠ è½½ç›¸åŒæ–‡ä»¶å¤¹æ—¶ä¼šè‡ªåŠ¨é‡æ–°åˆ›å»º")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {str(e)}")
                            else:
                                st.info("å½“å‰å‘é‡æ•°æ®åº“ä¸å­˜åœ¨")
                    else:
                        if st.button("ğŸ—‘ï¸ åˆ é™¤æ‰€æœ‰å‘é‡æ•°æ®åº“", use_container_width=True):
                            import shutil
                            if os.path.exists("./chroma_db"):
                                try:
                                    shutil.rmtree("./chroma_db")
                                    st.session_state.vectorstore = None
                                    st.success("âœ… æ‰€æœ‰å‘é‡æ•°æ®åº“å·²åˆ é™¤")
                                    st.info("ğŸ’¡ ä¸‹æ¬¡åŠ è½½æ–‡æ¡£æ—¶ä¼šè‡ªåŠ¨é‡æ–°åˆ›å»º")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {str(e)}")
                            else:
                                st.info("å‘é‡æ•°æ®åº“ä¸å­˜åœ¨")
                
                with col_vdb2:
                    # æ˜¾ç¤ºæ‰€æœ‰å‘é‡æ•°æ®åº“ä¿¡æ¯
                    if os.path.exists("./chroma_db"):
                        try:
                            import shutil
                            total_size = 0
                            db_count = 0
                            for item in os.listdir("./chroma_db"):
                                item_path = os.path.join("./chroma_db", item)
                                if os.path.isdir(item_path):
                                    db_count += 1
                                    total_size += sum(f.stat().st_size for f in Path(item_path).rglob('*') if f.is_file())
                            
                            size_mb = total_size / (1024 * 1024)
                            if db_count > 0:
                                st.caption(f"ğŸ’¾ å…± {db_count} ä¸ªå‘é‡æ•°æ®åº“")
                                st.caption(f"ğŸ’¾ æ€»å¤§å°: {size_mb:.2f} MB")
                            else:
                                st.caption("ğŸ’¾ å‘é‡æ•°æ®åº“å­˜åœ¨")
                        except:
                            st.caption("ğŸ’¾ å‘é‡æ•°æ®åº“å­˜åœ¨")
                    else:
                        st.caption("ğŸ’¾ æœªåˆ›å»ºå‘é‡æ•°æ®åº“")
                
                # æ˜¾ç¤º HuggingFace ç¼“å­˜ä¿¡æ¯
                hf_cache_path = os.path.join(os.path.expanduser("~"), ".cache", "huggingface")
                if os.path.exists(hf_cache_path):
                    try:
                        hf_size = sum(f.stat().st_size for f in Path(hf_cache_path).rglob('*') if f.is_file())
                        hf_size_gb = hf_size / (1024 * 1024 * 1024)
                        st.caption(f"ğŸ¤– HuggingFace æ¨¡å‹ç¼“å­˜: {hf_size_gb:.2f} GB")
                        st.caption(f"   ä½ç½®: {hf_cache_path}")
                        st.caption("   ğŸ’¡ å¦‚éœ€æ¸…ç†ï¼Œå¯æ‰‹åŠ¨åˆ é™¤è¯¥ç›®å½•")
                    except:
                        pass
        
        st.caption("ğŸ’¡ æç¤ºï¼šä½¿ç”¨æœ¬åœ°å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œæ— éœ€APIå¯†é’¥")
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("ğŸ“„ æ–‡æ¡£æµè§ˆå™¨")
        
        if st.session_state.docs:
            # æ–‡ä»¶åˆ—è¡¨
            files = list(st.session_state.docs.keys())
            selected_file = st.selectbox(
                "é€‰æ‹©æ–‡ä»¶æµè§ˆ",
                files,
                index=0,
                key="file_selector"
            )
            
            if selected_file:
                file_info = st.session_state.docs[selected_file]
                
                # æ–‡ä»¶ä¿¡æ¯å¡ç‰‡
                st.info(f"""
                **æ–‡ä»¶ä¿¡æ¯**
                - ç±»å‹: {file_info['type'].upper()}
                - å¤§å°: {file_info['size']:,} å­—èŠ‚
                """)
                
                # å†…å®¹æ˜¾ç¤º
                with st.expander("ğŸ“‹ æŸ¥çœ‹å†…å®¹", expanded=True):
                    content = file_info['content']
                    if isinstance(content, dict):  # Excelæ–‡ä»¶
                        tab_names = list(content.keys())
                        tabs = st.tabs(tab_names)
                        for i, (sheet_name, sheet_content) in enumerate(content.items()):
                            with tabs[i]:
                                st.text_area(
                                    f"{sheet_name} å·¥ä½œè¡¨",
                                    sheet_content,
                                    height=300,
                                    key=f"excel_{selected_file}_{i}"
                                )
                    else:
                        st.text_area(
                            "æ–‡ä»¶å†…å®¹",
                            content,
                            height=400,
                            key=f"content_{selected_file}"
                        )
        
        # æ‰¹é‡æ“ä½œ
        if st.session_state.docs:
            st.markdown("---")
            st.subheader("ğŸ“ˆ æ‰¹é‡æ€»ç»“")
            
            # æ–‡æ¡£é€‰æ‹©é€‰é¡¹
            summary_mode = st.radio(
                "é€‰æ‹©æ€»ç»“èŒƒå›´",
                ["ğŸ“š æ‰€æœ‰æ–‡æ¡£", "ğŸ“„ é€‰æ‹©ç‰¹å®šæ–‡æ¡£"],
                horizontal=True,
                help="é€‰æ‹©è¦æ€»ç»“çš„æ–‡æ¡£èŒƒå›´"
            )
            
            selected_files_for_summary = []
            if summary_mode == "ğŸ“„ é€‰æ‹©ç‰¹å®šæ–‡æ¡£":
                # å¤šé€‰æ–‡æ¡£
                file_options = list(st.session_state.docs.keys())
                selected_files_for_summary = st.multiselect(
                    "é€‰æ‹©è¦æ€»ç»“çš„æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰",
                    options=file_options,
                    default=[],
                    help="é€‰æ‹©è¦åŒ…å«åœ¨æ€»ç»“æŠ¥å‘Šä¸­çš„æ–‡æ¡£"
                )
                
                if not selected_files_for_summary:
                    st.info("ğŸ’¡ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡æ¡£")
                    summary_button_disabled = True
                else:
                    summary_button_disabled = False
                    st.info(f"âœ… å·²é€‰æ‹© {len(selected_files_for_summary)} ä¸ªæ–‡æ¡£")
            else:
                summary_button_disabled = False
                st.info(f"ğŸ“š å°†æ€»ç»“æ‰€æœ‰ {len(st.session_state.docs)} ä¸ªæ–‡æ¡£")
            
            # ç”ŸæˆæŠ¥å‘ŠæŒ‰é’®
            generate_summary_clicked = st.button(
                "ç”ŸæˆçŸ¥è¯†åº“æ€»ç»“æŠ¥å‘Š", 
                use_container_width=True,
                disabled=summary_button_disabled if summary_mode == "ğŸ“„ é€‰æ‹©ç‰¹å®šæ–‡æ¡£" else False
            )
            
            # å¤„ç†ç”Ÿæˆæ€»ç»“æŠ¥å‘Šçš„é€»è¾‘
            if generate_summary_clicked:
                if not api_key:
                    st.error("è¯·å…ˆè¾“å…¥DeepSeek APIå¯†é’¥")
                else:
                    # ç¡®å®šè¦æ€»ç»“çš„æ–‡æ¡£
                    if summary_mode == "ğŸ“š æ‰€æœ‰æ–‡æ¡£":
                        files_to_summarize = None  # None è¡¨ç¤ºæ‰€æœ‰æ–‡æ¡£
                        summary_title = f"æ‰€æœ‰æ–‡æ¡£æ€»ç»“ï¼ˆå…± {len(st.session_state.docs)} ä¸ªæ–‡æ¡£ï¼‰"
                    else:
                        files_to_summarize = selected_files_for_summary
                        summary_title = f"é€‰å®šæ–‡æ¡£æ€»ç»“ï¼ˆå…± {len(selected_files_for_summary)} ä¸ªæ–‡æ¡£ï¼‰"
                    
                    with st.spinner(f"æ­£åœ¨ç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼ˆ{summary_title}ï¼‰..."):
                        summary = generate_summary_deepseek(
                            st.session_state.docs, 
                            api_key,
                            specific_files=files_to_summarize
                        )
                        # ä¿å­˜åˆ° session state
                        st.session_state.summary = summary
                        st.session_state.summary_title = summary_title
                        # ä¿å­˜æ–‡æ¡£åˆ—è¡¨ä¿¡æ¯ç”¨äºåç»­æ˜¾ç¤º
                        if files_to_summarize:
                            st.session_state.summary_files = files_to_summarize
                            st.session_state.summary_doc_count = len(files_to_summarize)
                        else:
                            st.session_state.summary_files = None
                            st.session_state.summary_doc_count = len(st.session_state.docs)
            
            # æ˜¾ç¤ºæ€»ç»“æŠ¥å‘Šï¼ˆç§»åˆ°æŒ‰é’®ifå—å¤–ï¼Œç¡®ä¿å§‹ç»ˆæ˜¾ç¤ºåœ¨col1ï¼‰
            if 'summary' in st.session_state and st.session_state.summary:
                st.markdown("---")
                summary_title_display = st.session_state.get('summary_title', 'æ€»ç»“æŠ¥å‘Š')
                with st.expander(f"ğŸ“Š æŸ¥çœ‹æ€»ç»“æŠ¥å‘Š - {summary_title_display}", expanded=True):
                    # æ˜¾ç¤ºæ€»ç»“çš„æ–‡æ¡£ä¿¡æ¯
                    summary_files = st.session_state.get('summary_files', None)
                    summary_doc_count = st.session_state.get('summary_doc_count', 0)
                    if summary_files:
                        st.markdown(f"**æ€»ç»“çš„æ–‡æ¡£ï¼š** {', '.join(summary_files)}")
                    else:
                        st.markdown(f"**æ€»ç»“çš„æ–‡æ¡£ï¼š** æ‰€æœ‰æ–‡æ¡£ï¼ˆå…± {summary_doc_count} ä¸ªï¼‰")
                    st.markdown("---")
                    st.write(st.session_state.summary)
                
                # ä¿å­˜æŠ¥å‘Šé€‰é¡¹
                st.markdown("#### ğŸ’¾ ä¿å­˜æŠ¥å‘Š")
                col_save1, col_save2, col_save3 = st.columns(3)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # ç¡®å®šæ–‡æ¡£åˆ—è¡¨ç”¨äºä¿å­˜
                summary_files = st.session_state.get('summary_files', None)
                summary_doc_count = st.session_state.get('summary_doc_count', 0)
                if summary_files:
                    doc_list = summary_files
                    doc_count = len(summary_files)
                else:
                    doc_list = list(st.session_state.docs.keys())
                    doc_count = summary_doc_count
                
                # ç”Ÿæˆ Markdown æ ¼å¼å†…å®¹
                md_summary = f"# {summary_title_display}\n\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\næ–‡æ¡£æ•°é‡: {doc_count}\n\n---\n\n{st.session_state.summary}"
                
                with col_save1:
                    st.download_button(
                        label="ğŸ“„ ä¿å­˜ä¸ºTXT",
                        data=st.session_state.summary,
                        file_name=f"çŸ¥è¯†åº“æ€»ç»“_{timestamp}.txt",
                        mime="text/plain",
                        use_container_width=True
                    )
                
                with col_save2:
                    st.download_button(
                        label="ğŸ“ ä¿å­˜ä¸ºMarkdown",
                        data=md_summary,
                        file_name=f"çŸ¥è¯†åº“æ€»ç»“_{timestamp}.md",
                        mime="text/markdown",
                        use_container_width=True
                    )
                
                with col_save3:
                    # JSONæ ¼å¼ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
                    json_data = {
                        "æ€»ç»“æ ‡é¢˜": summary_title_display,
                        "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "æ–‡æ¡£æ•°é‡": doc_count,
                        "æ–‡æ¡£åˆ—è¡¨": doc_list,
                        "æ€»ç»“å†…å®¹": st.session_state.summary
                    }
                    st.download_button(
                        label="ğŸ“Š ä¿å­˜ä¸ºJSON",
                        data=json.dumps(json_data, ensure_ascii=False, indent=2),
                        file_name=f"çŸ¥è¯†åº“æ€»ç»“_{timestamp}.json",
                        mime="application/json",
                        use_container_width=True
                    )
                
                # è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°ï¼ˆå¯é€‰ï¼‰
                save_dir = os.path.join(".", "saved_reports")
                os.makedirs(save_dir, exist_ok=True)
                
                if st.checkbox("ğŸ’¾ åŒæ—¶è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°", value=False, key="auto_save_summary"):
                    try:
                        # ä¿å­˜TXTç‰ˆæœ¬
                        txt_path = os.path.join(save_dir, f"çŸ¥è¯†åº“æ€»ç»“_{timestamp}.txt")
                        with open(txt_path, 'w', encoding='utf-8') as f:
                            f.write(st.session_state.summary)
                        
                        # ä¿å­˜Markdownç‰ˆæœ¬
                        md_path = os.path.join(save_dir, f"çŸ¥è¯†åº“æ€»ç»“_{timestamp}.md")
                        with open(md_path, 'w', encoding='utf-8') as f:
                            f.write(md_summary)
                        
                        st.success(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}")
                        st.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„:\n- {txt_path}\n- {md_path}")
                    except Exception as e:
                        st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
    
    with col2:
        st.header("ğŸ¤– æ™ºèƒ½é—®ç­”")
        
        # èŠå¤©å†å²
        if st.session_state.chat_history:
            with st.expander("ğŸ—£ï¸ å¯¹è¯å†å²", expanded=False):
                # ä¿å­˜å¯¹è¯å†å²æŒ‰é’®
                col_history1, col_history2 = st.columns([3, 1])
                with col_history2:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # ç”Ÿæˆå¯¹è¯å†å²æ–‡æœ¬
                    history_text = f"# å¯¹è¯å†å²è®°å½•\n\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\næ–‡æ¡£æ•°é‡: {len(st.session_state.docs)}\n\n---\n\n"
                    for i, (q, a) in enumerate(st.session_state.chat_history, 1):
                        history_text += f"## å¯¹è¯ {i}\n\n**é—®é¢˜:** {q}\n\n**å›ç­”:**\n{a}\n\n---\n\n"
                    
                    st.download_button(
                        label="ğŸ’¾ ä¿å­˜å¯¹è¯",
                        data=history_text,
                        file_name=f"å¯¹è¯å†å²_{timestamp}.md",
                        mime="text/markdown",
                        use_container_width=True
                    )
                
                # æ˜¾ç¤ºæœ€è¿‘5æ¡å¯¹è¯
                for i, (q, a) in enumerate(st.session_state.chat_history[-5:], start=len(st.session_state.chat_history)-4):  # æ˜¾ç¤ºæœ€è¿‘5æ¡
                    st.markdown(f"**Q{i}:** {q}")
                    st.markdown(f"**A{i}:** {a}")
                    st.markdown("---")
        
        # é—®é¢˜è¾“å…¥
        question = st.text_area(
            "è¾“å…¥æ‚¨çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼šæ€»ç»“ä¸€ä¸‹æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿæˆ–è€…ï¼šä»è¿™äº›æ–‡æ¡£ä¸­æ‰¾å‡ºå…³äºXXçš„ä¿¡æ¯ã€‚",
            height=100,
            key="question_input"
        )
        
        col_a, col_b, col_c = st.columns([1, 1, 2])
        
        with col_a:
            search_clicked = st.button("ğŸ” æœç´¢ç­”æ¡ˆ", type="primary", use_container_width=True)
        
        with col_b:
            if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                # æ¸…ç©ºå‰è¯¢é—®æ˜¯å¦ä¿å­˜
                if st.session_state.chat_history:
                    st.warning("âš ï¸ æ¸…ç©ºå¯¹è¯å‰å»ºè®®å…ˆä¿å­˜å¯¹è¯å†å²ï¼")
                st.session_state.chat_history = []
                st.rerun()
        
        with col_c:
            if st.button("ğŸ’¡ ç¤ºä¾‹é—®é¢˜", use_container_width=True):
                examples = [
                    "æ€»ç»“æ‰€æœ‰æ–‡æ¡£çš„æ ¸å¿ƒè¦ç‚¹",
                    "æå–æ–‡æ¡£ä¸­çš„å…³é”®æ•°æ®",
                    "åˆ—å‡ºæ‰€æœ‰æåˆ°çš„é‡è¦æ—¥æœŸ",
                    "å„æ–‡æ¡£ä¹‹é—´çš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ"
                ]
                st.session_state.question_input = st.session_state.get("question_input", "") + examples[0]
        
        # å¤„ç†æœç´¢ç­”æ¡ˆçš„é€»è¾‘ï¼ˆç§»åˆ°åˆ—å¸ƒå±€å¤–ï¼Œä½¿å†…å®¹å æ®å…¨å®½ï¼‰
        if search_clicked:
            # æ¸…é™¤é«˜çº§åŠŸèƒ½æ˜¾ç¤ºçŠ¶æ€
            st.session_state.show_data_analysis = False
            
            if not question:
                st.warning("è¯·è¾“å…¥é—®é¢˜")
            elif not api_key:
                st.error("è¯·è¾“å…¥DeepSeek APIå¯†é’¥")
            elif not st.session_state.docs:
                st.error("è¯·å…ˆåŠ è½½æ–‡æ¡£")
            else:
                # æ˜¾ç¤ºè¶…æ—¶æç¤ºï¼ˆå…¨å®½ï¼‰
                timeout_info = st.session_state.get('api_timeout', 60)
                retry_info = st.session_state.get('api_max_retries', 3)
                st.info(f"â±ï¸ è¶…æ—¶è®¾ç½®: {timeout_info}ç§’ | é‡è¯•æ¬¡æ•°: {retry_info}æ¬¡ | å¦‚é‡è¶…æ—¶å¯åœ¨ä¾§è¾¹æ è°ƒæ•´")
                
                with st.spinner(f"æ­£åœ¨æ€è€ƒ...ï¼ˆè¶…æ—¶æ—¶é—´: {timeout_info}ç§’ï¼‰"):
                    answer = answer_with_deepseek(
                        question, 
                        st.session_state.vectorstore, 
                        st.session_state.docs, 
                        api_key
                    )
                    
                    # ä¿å­˜åˆ°å†å²
                    st.session_state.chat_history.append((question, answer))
                    
                    # æ˜¾ç¤ºç­”æ¡ˆï¼ˆå…¨å®½ï¼‰
                    st.markdown("### ğŸ’¡ ç­”æ¡ˆ")
                    st.write(answer)
                    
                    # ä¿å­˜å•ä¸ªé—®ç­”
                    col_save_qa1, col_save_qa2 = st.columns(2)
                    timestamp_qa = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # ç”Ÿæˆé—®ç­”å†…å®¹
                    qa_text = f"# é—®ç­”è®°å½•\n\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## é—®é¢˜\n\n{question}\n\n## å›ç­”\n\n{answer}\n"
                    
                    with col_save_qa1:
                        st.download_button(
                            label="ğŸ’¾ ä¿å­˜æ­¤é—®ç­”",
                            data=qa_text,
                            file_name=f"é—®ç­”_{timestamp_qa}.md",
                            mime="text/markdown",
                            use_container_width=True
                        )
                    
                    with col_save_qa2:
                        # è‡ªåŠ¨ä¿å­˜é€‰é¡¹
                        if st.checkbox("è‡ªåŠ¨ä¿å­˜", key=f"auto_save_{timestamp_qa}", value=False):
                            save_dir_qa = os.path.join(".", "saved_qa")
                            os.makedirs(save_dir_qa, exist_ok=True)
                            try:
                                qa_path = os.path.join(save_dir_qa, f"é—®ç­”_{timestamp_qa}.md")
                                with open(qa_path, 'w', encoding='utf-8') as f:
                                    f.write(qa_text)
                                st.success(f"âœ… å·²ä¿å­˜åˆ°: {save_dir_qa}")
                            except Exception as e:
                                st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
                    
                    # å¦‚æœç­”æ¡ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œæä¾›è§£å†³å»ºè®®
                    if "è¶…æ—¶" in answer or "è¿æ¥" in answer or "ç½‘ç»œ" in answer:
                        with st.expander("ğŸ’¡ ç½‘ç»œé—®é¢˜è§£å†³å»ºè®®", expanded=True):
                            st.markdown("""
                            **å¦‚æœé‡åˆ°è¶…æ—¶æˆ–è¿æ¥é—®é¢˜ï¼Œå¯ä»¥å°è¯•ï¼š**
                            1. ğŸ“ˆ **å¢åŠ è¶…æ—¶æ—¶é—´**ï¼šåœ¨ä¾§è¾¹æ "é«˜çº§è®¾ç½®"ä¸­å¢åŠ è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®®120-180ç§’ï¼‰
                            2. ğŸ”„ **å¢åŠ é‡è¯•æ¬¡æ•°**ï¼šåœ¨ä¾§è¾¹æ "é«˜çº§è®¾ç½®"ä¸­å¢åŠ é‡è¯•æ¬¡æ•°ï¼ˆå»ºè®®4-5æ¬¡ï¼‰
                            3. ğŸŒ **æ£€æŸ¥ç½‘ç»œ**ï¼šç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®šï¼Œå¯ä»¥è®¿é—® api.deepseek.com
                            4. ğŸ“ **å‡å°‘æ–‡æ¡£å†…å®¹**ï¼šå¦‚æœæ–‡æ¡£å¾ˆå¤§ï¼Œå°è¯•å‡å°‘åŠ è½½çš„æ–‡æ¡£æ•°é‡
                            5. â° **ç¨åé‡è¯•**ï¼šå¯èƒ½æ˜¯DeepSeekæœåŠ¡å™¨ç¹å¿™ï¼Œç¨åå†è¯•
                            """)
                    
                    # æ˜¾ç¤ºæ£€ç´¢æ¥æº
                    if st.session_state.vectorstore:
                        with st.expander("æŸ¥çœ‹å‚è€ƒæ¥æº"):
                            similar_docs = search_similar_documents(
                                st.session_state.vectorstore, 
                                question
                            )
                            for i, (content, source) in enumerate(similar_docs[:3], 1):
                                st.markdown(f"**æ¥æº {i} - {source}**")
                                st.caption(content[:300] + "...")
        
        # é«˜çº§åŠŸèƒ½ï¼ˆåœ¨col2å†…ï¼Œç¡®ä¿å¸ƒå±€æ­£ç¡®ï¼‰
        st.markdown("---")
        st.subheader("ğŸ¯ é«˜çº§åŠŸèƒ½")
        
        # æ•°æ®åˆ†ææŒ‰é’®ï¼ˆè¯­ä¹‰æœç´¢åŠŸèƒ½å·²ç§»é™¤ï¼Œå› ä¸ºä¸"æŸ¥çœ‹å‚è€ƒæ¥æº"åŠŸèƒ½é‡å¤ï¼‰
        data_analysis_clicked = st.button("ğŸ“Š æ•°æ®åˆ†æ", use_container_width=True, key="data_analysis_btn")
        
        # åˆå§‹åŒ–session_state
        if 'show_data_analysis' not in st.session_state:
            st.session_state.show_data_analysis = False
        
        if data_analysis_clicked:
            st.session_state.show_data_analysis = True
        
        # å¤„ç†æ•°æ®åˆ†æï¼ˆåœ¨col2å†…ï¼Œä½¿ç”¨å®¹å™¨ç»„ç»‡ç»“æœï¼‰
        if st.session_state.show_data_analysis:
            if not st.session_state.docs:
                st.warning("è¯·å…ˆåŠ è½½æ–‡æ¡£")
            elif not api_key:
                st.error("è¯·è¾“å…¥DeepSeek APIå¯†é’¥")
            else:
                with st.spinner("æ­£åœ¨åˆ†ææ–‡æ¡£..."):
                    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£é›†åˆï¼Œæä¾›æ•°æ®åˆ†æ:

æ–‡æ¡£ä¿¡æ¯ï¼š
{chr(10).join([f'{name}: {len(str(data["content"]))} å­—ç¬¦' for name, data in st.session_state.docs.items()])}

è¯·æä¾›ï¼š
1. æ–‡æ¡£å†…å®¹åˆ†å¸ƒåˆ†æ
2. æ½œåœ¨çš„æ•°æ®æ¨¡å¼å’Œè¶‹åŠ¿
3. å»ºè®®çš„æ•°æ®å¯è§†åŒ–æ–¹å¼"""
                    
                    analysis = query_deepseek(prompt, api_key)
                    st.markdown("### ğŸ“Š æ•°æ®åˆ†æç»“æœ")
                    st.write(analysis)
        
        # æ˜¾ç¤ºç‰ˆæƒä¿¡æ¯
        show_footer()

# ç®€æ˜“ç‰ˆï¼ˆæ— å‘é‡æ•°æ®åº“ï¼‰
def simple_main():
    """ç®€æ˜“ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨å‘é‡æ•°æ®åº“"""
    st.set_page_config(
        page_title="ç®€æ˜“çŸ¥è¯†åº“æµè§ˆå™¨",
        page_icon="ğŸ“",
        layout="wide"
    )
    
    st.title("ğŸ“ ç®€æ˜“çŸ¥è¯†åº“æµè§ˆå™¨")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒtxt, docx, pdf, excel)",
        type=['txt', 'docx', 'pdf', 'xlsx', 'xls'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        docs = {}
        temp_dir = tempfile.mkdtemp()
        for uploaded_file in uploaded_files:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # è¯»å–æ–‡ä»¶
            filename = uploaded_file.name
            file_ext = filename.split('.')[-1].lower()
            
            try:
                if file_ext == 'txt':
                    content = read_text_file(file_path)
                elif file_ext == 'docx':
                    content = read_docx_file(file_path)
                elif file_ext == 'pdf':
                    content = read_pdf_file(file_path)
                elif file_ext in ['xlsx', 'xls']:
                    content = read_excel_file(file_path)
                else:
                    content = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"
                
                docs[filename] = {
                    'content': content,
                    'type': file_ext,
                    'path': file_path
                }
            except Exception as e:
                st.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
        
        # æ–‡ä»¶æµè§ˆå™¨
        if docs:
            selected_file = st.selectbox("é€‰æ‹©æ–‡ä»¶", list(docs.keys()))
            
            if selected_file in docs:
                content = docs[selected_file]['content']
                st.text_area("æ–‡ä»¶å†…å®¹", 
                           str(content) if not isinstance(content, dict) else str(content),
                           height=400)
        
        # ç®€å•é—®ç­”
        st.markdown("---")
        
        # å°è¯•åŠ è½½ä¿å­˜çš„ API key
        saved_api_key = load_api_key()
        default_key = saved_api_key if saved_api_key else ""
        
        col_simple_key1, col_simple_key2 = st.columns([3, 1])
        with col_simple_key1:
            api_key = st.text_input("DeepSeek APIå¯†é’¥", value=default_key, type="password")
        with col_simple_key2:
            if st.button("ğŸ’¾ ä¿å­˜", use_container_width=True):
                if api_key:
                    if save_api_key(api_key):
                        st.success("âœ… å·²ä¿å­˜")
                else:
                    st.warning("è¯·è¾“å…¥å¯†é’¥")
        
        question = st.text_input("è¾“å…¥é—®é¢˜")
        
        if st.button("è·å–ç­”æ¡ˆ") and api_key and question:
            # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_content = ""
            for filename, data in docs.items():
                content = data['content']
                if isinstance(content, dict):
                    content = "\n".join([f"{k}: {v}" for k, v in content.items()])
                all_content += f"\n\næ–‡ä»¶: {filename}\n{content}"
            
            # è°ƒç”¨DeepSeek
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

{all_content[:8000]}

é—®é¢˜ï¼š{question}

è¯·åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
            
            with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                answer = query_deepseek(prompt, api_key)
                st.write("**ç­”æ¡ˆï¼š**", answer)
        
        # æ˜¾ç¤ºç‰ˆæƒä¿¡æ¯
        show_footer()

if __name__ == "__main__":
    # é»˜è®¤ä½¿ç”¨å®Œæ•´ç‰ˆï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°åˆ‡æ¢
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "simple":
        simple_main()
    else:
        main()